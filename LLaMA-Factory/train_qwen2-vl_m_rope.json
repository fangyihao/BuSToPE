{
  "deepspeed": "ds_z3_config.json",
  "model_name_or_path": "models--Qwen--Qwen2-VL-2B-Instruct",
  "stage": "sft",
  "total_pixels": 6272000,
  "video_maxlen": 128,
  "do_train": true,
  "finetuning_type": "full",
  "dataset": "llava_videos_330k_split0,llava_videos_330k_split1,llava_videos_330k_split2",
  "seed": 42,
  "template": "qwen2_vl",
  "cutoff_len": 8200,
  "overwrite_cache": true,
  "preprocessing_num_workers": 32,
  "output_dir": "checkpoints/models--Qwen--Qwen2-VL-2B-Instruct/m_rope",
  "num_train_epochs": 1.0,
  "logging_steps": 1,
  "save_steps": 1,
  "save_total_limit": 1,
  "plot_loss": true,
  "overwrite_output_dir": true,
  "per_device_train_batch_size": 1,
  "gradient_accumulation_steps": 128,
  "learning_rate": 1e-05,
  "lr_scheduler_type": "cosine",
  "warmup_ratio": 0.1,
  "bf16": true,
  "ddp_timeout": 180000000,
  "val_size": 1,
  "per_device_eval_batch_size": 1,
  "eval_strategy": "steps",
  "eval_steps": 1,
  "flash_attn": "fa2",
  "resume_from_checkpoint": "checkpoints/models--Qwen--Qwen2-VL-2B-Instruct/m_rope/checkpoint-180",
  "which_rope": "m_rope",
  "report_to": "none",
  "tokenized_path": "tokenized/models--Qwen--Qwen2-VL-2B-Instruct/m_rope"
}